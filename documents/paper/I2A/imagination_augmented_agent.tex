\section{Imagination-augmented agent} 
\label{sec:i2a} 
 
The paper "Imagination-augmented agents for deep reinforcement learning" from Weber et al. \cite{I2A} combines the advantages of model-free and model-based reinforcement learning to get an agent which is robust against model imperfections, but is able to use the advantages of model-based agents.
The imagination-augmented agent learns therefore to combine information from a model-free and a model-based imagination-augmented path.\\

%To do this they train a model of the environment for internal simulations. 
%Imagine the future and learn to do better actions based on the imagined future.\\ 
 
Figure \ref{fig:i2a_architecture} shows the network architecture of the imagination-augmented agent, which will be explained in the following.

\begin{figure}[H] 
  \centering 
   
  \includegraphics[width=\columnwidth]{./Images/i2a_architecture.pdf} 
  \caption{Network architecture for deep reinforcement learning which combines model free and model based reinforcement learning. a) imagination core (IC) predicts the next time step conditioned on an action sampled from the rollout policy $\hat{\pi}$. b) the imagination core imagines trajectories of features f encoded by the rollout encoder. c) the full i2a} 
  \label{fig:i2a_architecture} 
\end{figure}

 
\subsection{Imagination core}

%The \textbf{imagination core} (Figure \ref{fig:i2a_architecture} a) imagine the next observation $\hat{o}_{t+1}$ and the next reward $\hat{r}_{t+1}$ given the observation $o_t$ or $\hat{o}_{t}$, where $o_t$ refers to the  input i2a get and $\hat{o}_{t}$ refers to an internal state of i2a which is an output of a previouse rollout with the imagination core. The pair of next observation $\hat{o}_{t+1}$ and the next reward $\hat{r}_{t+1}$ is called features $\hat{f} = (\hat{o}, \hat{r})$\\

The \textbf{imagination core} (Figure \ref{fig:i2a_architecture} a) imagines trajectories of features $\hat{f}_{t+1} = (\hat{o}_{t+1}, \hat{r}_{t+1})$, containing the next observation $\hat{o}_{t+1}$ and the next reward $\hat{r}_{t+1}$, given the observation $o_t$ or $\hat{o}_{t}$, where $o_t$ refers to the input i2a gets and $\hat{o}_{t}$ refers to an internal state of i2a, which is an output of a previous rollout with the imagination core.
To do so the imagination core uses a rollout policy, which predicts an action given the current state, and an environment model (see chapter \ref{sec:env_model}), which predicts the next observation and the next reward.\\

%$o_t$: initial observation  $\hat{o}_t$: predicted observation  $\hat{r}_t$: predicted reward Given observation $o_t$ or $\hat{o}_t$ and action $\hat{a}_t$ predict (imagine) the next observation $\hat{o}_{t+1}$ and next reward $\hat{r}_{t+1}$  

%To do so the imagination core uses a rollout policy, which decides the next action $\hat{a}_t$, and an environment model, which predicts the next state and the reward signals from the environment, given an state and a current action.\\


The \textbf{rollout policy $\hat{\pi}$} (Figure \ref{fig:i2a_architecture} a) is a model-free reinforcement learning agent which should, given the same observation, predict the same action as the i2a policy $\pi$ (Figure \ref{fig:i2a_architecture} c). To ensure this, the rollout policy needs to be similar to the i2a policy $\pi$, which can be ensured by minimizing the distillation loss, the cross entropy between the rollout policy $\hat{\pi}$ and the i2a policy $\pi$:

%Distillation loss Make $\hat{\pi}$ (rollout policy) and $\pi$ (i2a policy) similar\\ 
 
\begin{equation} 
    l_{dist}(\pi, \hat{\pi})(o_t) = \lambda_{dist} \sum_a \pi(a | o_t) log \hat{\pi}(a|o_t) 
\end{equation} 

with scaling parameter $\lambda_{dist}$.\\
% $\lambda_{dist}$ is set to 10


\subsection{Environment Model}
\label{sec:env_model}

The environment model (Figure \ref{fig:i2a_architecture} a) predicts (imagines) the next observation $\hat{o}_{t+1}$ and next reward $\hat{r}_{t+1}$, given observation $o_t$ or $\hat{o}_t$ and action $\hat{a}_t$.
The environment model could be any kind of model that fulfills this condition, but in most cases no model of the environment is given.
To solve this problem the environment model is also a trainable neural network.
%, which is trained with enrolled pairs of (state, action) transitions made in the environment the i2a agent will act on.\\
A trained environment model can not be assumed to be perfect, it might sometimes make wrong predictions, but this does not pose a problem, as the imagination augmented agent is robust to imperfect environment models.

   
\begin{figure}[H] 
  \centering 
   
  \includegraphics[width=300px]{./Images/i2a_env.pdf}
  \caption{Environment model. The input action is broadcasted and concatenated to the observation. A convolutional neural network then processes the input and predicts the next observation and the expected reward.} 
  \label{fig:environment_model_architecture} 
\end{figure} 

Figure \ref{fig:environment_model_architecture} shows the environment model architecture.
It gets as input the last observation $o_t$ as rgb image and an action $a$, which was selected from the rollout policy $\hat{\pi}$. The action is converted into a one-hot vector and tiled to the size of the observation.
The tiled action is then concatenated with the input observations $o_t$ and named stacked context.
The number of channels of the stacked context is now equal to three rgb channels plus the number of possible actions.
Given the stacked context as input a convolutional neural network with two heads predicts the next observation $o_{t+1}$ and the expected reward $r_{t+1}$.\\

The neural network is trained with transitions of the form $(o_t, a_t) \rightarrow (o_{t+1}, r_{t+1})$ generated from a pretrained model-free advantage-actor-critic policy. This is necessary because a random agent is not able to generate a representative set of states, as it sees few rewards in some of the domains.\\

 
The observation head of the environment model is trained by maximizing the log-likelihood of the probability $p(o_t | a_{t-1}, o_{t-1})$, which is a bernoulli distribution given by: 
\begin{equation} 
   p(o_t | a_{t-1}, o_{t-1}) = x^y (1-x)^{1-y} 
\end{equation}

A bernoulli distribution works well because \textbf{TODO}\\

%Basically we pretend that x and \hat x are both a collection of independent bernoulli probabilities (even if they are really just number in between 0 and 1).
   
Maximizing the log-likelihood of the probability $p(o_t | a_{t-1}, o_{t-1})$ is the same as the binary cross entropy loss between the true image and the predicted image:

%The loss for training the observation of the environment model is therefor the same as the binary cross entropy between the true image and the predicted image:
   
\begin{equation} 
  \mathnormal{ 
  L_{env}(x, y) = \frac{1}{N} \sum y_n log x_n + (1-y_n) log(1- x_n) 
  %env_{loss} = Binary Cross Entropy(predicted\_image, true\_images) 
  } 
\end{equation}

To train the predicted reward a L2 loss is used.\\

TODO the env model can be fine tuned with the i2a
 
\subsection{Rollout Encoder}
 

The rollout encoder processes the imagined rollout as a whole and learns to interpret it, by using any useful information and ignoring information when necessary, see Figure \ref{fig:i2a_architecture}b.
To do so each rollout encoder predicts a imagined trajectory $T = (\hat{f}_{t+1}, ... \hat{f}_{t+n})$, which is a sequence of features, by performing $n$ rollouts, given the input observation $o_t$ and a start action $a_t$.
%To do so each rollout encoder gets as input the input observation $o_t$ and a start action $a_t$ and performs $n$ rollouts by predicting a sequence of n features $\hat{f}_{t+i} = (\hat{o}_{t+i}, \hat{r}_{t+i})$ for $i = 0,...n$, this sequence of features $(\hat{f}_{t+1}, ... \hat{f}_{t+n})$ is called a imagined trajectory $T$.\\
The encoder consists of a convolutional neural network (CNN) followed by an long-short-term-memory (LSTM) network which processes the information given by the imagined trajectory $T$. By using an LSTM network the encoder is able to learn long-term dependencies in the rollouts.\\

 
%The imagination core imagines trajectories of features $f = (\hat{o}, \hat{r})$\\ 
%The rollout encoder encode these trajectories     
%CNN Network followed by an LSTM Network 
%CNN Network:    Encode observation and reward $\hat{o}_{t+i}, \hat{r}_{t+i}$ 
%LSTM Network:    Learns long-term dependencies 
   
 
\subsection{I2A Architecture}


As described above the I2A architecture combines model-free and model-based reinforcement learning (figure \ref{fig:i2a_architecture}c).
The model-based path performs for each action $a$ the agent can take an imagination rollout with the rollout encoder.
The aggregator then concatenates all rollouts.
The model-free path is just a neural network with CNN layers followed by fully connected (FC) layers.
If only the model-free path would be used the architecture would be equal to a model-free reinforcement learning architecture.\\
%advantage-actor-critic architecture.\\
 

The outputs of the model-free path and the model-based path are then simply concatenated and passed to an output policy network which consists of one FC layer and two heads, the policy head $\pi$ and the value head $V$.
As I2A is just an architecture design it can be trained with advantage-actor-critic as described in section \ref{sec:a2c}.\\

%For each action $a$ the agent can take do a imagination rollout  
% the future based on a model of the environment and use this information for decision making 
 
 
%I2A Architecture - Model Based Path  For each action $a$ the agent can take, do a imagination rollout     Aggregator:   Concatinate all action rollouts    

%I2A Architecture - Model Free Path CNN Layers followed by Fully Connected Layers 

 
%I2A Training 
%Input: Observation $o_t$ 
%Output:  Policy $\pi$ and value function $V$   Train with Advantage-Actor-Critic (A2C) 

 
\subsection{Model-free baseline}

For the model-free baseline an a2c architecture is used consisting of two convolutional layers followed by a FC layer and two FC output heads, one for the policy and one for the value function.\\

\subsection{Copy Model}

To verify that the improvement of the i2a agent does not rely on the higher model complexity, a copy model is used for comparison. The copy model has the same network architecture as the i2a model but instead of using an environment model a 'copy' model is used which simply returns the input observation. 