\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{How to use Imagination-Augmented Agents for Deep Reinforcement Learning}

\author{Angela Denninger\\
{\tt\small angela.denninger@tum.de}
\and
Florian Klemt\\
{\tt\small florian.klemt@tum.de}
\and
Felix Schober\\
{\tt\small felix.schober@tum.de}
\and
Max-Philipp Schrader\\
{\tt\small max.schrader@manageandmore.de}
%\and
%Team Member 5\\
%{\tt\small fifth@i1.org}
}


\maketitle
%\thispagestyle{empty}

%
% Proposal I
%
\section*{Project Proposal}

\section{Introduction}
    During this years NIPS 2017 conference the paper \textit{Imagination-Augmented Agents for Deep Reinforcement Learning} \cite{NIPS2017_7152} was presented by a team of researchers at DeepMind. In the paper a novel architecture for deep reinforcement learning, called Imagination-Augmented Agents (I2As), was proposed. The I2A architecture has shown improvements on performance and robustness as well as data efficiency compared to other approaches
   
    Within the scope of our project we want to develop an implementation of I2As and compare it with a simple Reinforcement Learning algorithm also implemented by out team.
    We were not able to find any implementation of the I2A architecture. Therefore, our goal is to examine the I2A architecture and provide reusable code to the public.

    \subsection{Related Works}
    There have been a many publications and implementations of Reinforcement Learning for Atari games based only on pixel input. The first and most known work has been published by DeepMind in 2013 \cite{DBLP:journals/corr/MnihKSGAWR13}. Following this approach several researches published work in this area of Reinforcement Learning \cite{guo2014deep,mnih2015human,schulman2015trust,hausknecht2015deep,DBLP:journals/corr/WangFL15}. One of the most recent publications is our input paper published during this years NIPS conference \cite{NIPS2017_7152}, which introduced a novel deep reinforcement learning architecture. Within the paper the researchers use the standard free-model presented by \cite{mnih2016asynchronous} as their baseline.
        %\begin{itemize}
            %\item \textit{Imagination-Augmented Agents for Deep Reinforcement Learning} \cite{NIPS2017_7152}
            %\item Mehr Literatur
            %\item Related and previous work on your topic
            %\item A small overview of the SOTA (state-of-the-art)
            %\item What is new/different in your approach?
            %\item $\dots$
        %\end{itemize}

\section{Dataset}
    For developing and benchmarking our Deep Reinforcement implementation we will be using traditional Atari Games. Therefore, we will work with the Atari games environment provided by OpenAI Gym \cite{openai}. The OpenAI Gym allows us to get the current state of the game, including the display and a reward system. In addition, we can perform actions within the environment to change the state. The input for our network will be the current state of the environment whereas the output will be the action to be performed next.
    \begin{itemize}
        \item OpenAI Gym \cite{openai}
        \item \textbf{Input:} Environment State of the Atari game, where the main input is the pixel map.
        \item \textbf{Output:} Next action to be performed in the environment
        %
        %\item \textit{Deren Input}
        %\item Are you working with an existing dataset or is data collection part of your project?
        %\item Explain the general nature of your data (show examples if beneficial) and provide information how you collected your dataset
        %\item Does your data provide the labels necessary for training?
        %\item Explain possible problems with the dataset
        %\item What are your inputs and outputs?
        %\item $\dots$
    \end{itemize}

\section{Methodology}
    First, we will develop a simple reinforcement learning model (RLM), which we will later use as a baseline to compare it against our implementation of the I2A architecture. In the next step we will implement a model based on the I2A architecture. 

    One of the more difficult parts for us during the planing process was the task distribution. We thought about the following work packages:
    
    \begin{enumerate}
    	\item OpenAI Gym Setup/Training
    	\item baseline implementation
    	\item I2A architecture implementation
    	\item Analysis of results and presentation 
    \end{enumerate}
    	
    	
  	 Where 3. would be done by two people. Overall we are not sure if our planing is perfect and we would love to have your feedback on this one.
   
   	During the project we will follow these steps
    \begin{itemize}
        \item Implementation of a classical RLM playing Atari games
        \item Implementation of the I2A architecture
        \item Comparing our I2A implementation to the RLM baseline and the results published at \cite{NIPS2017_7152} on the game Sokoban
        \item Learning to play a subset of all 2600 Atari games
        %\item \textit{Deren Input}
        %\item Network architecture(s)
        %\item Transfer learning and training from scratch
        %\item Resource management (please consider GPU memory)
        %\item $\dots$
    \end{itemize}

\section{Outcome}
    At the end of our project our goal is to have working implementation of the I2A architecture, which is able to outperform our baseline implementation. As an additional goal we will try to get as close as possible to the published results in the paper \cite{NIPS2017_7152}. Due to the novelty of the paper we can not guaranty to have the I2A architecture implemented. However, we are positive to reach our goals and will do our best during the project.

{\small
\bibliographystyle{ieee}
\bibliography{bib}
}

\end{document}