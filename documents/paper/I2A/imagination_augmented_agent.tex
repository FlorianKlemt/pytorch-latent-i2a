\section{Imagination-augmented agent} 
\label{sec:i2a} 
 
The paper "Imagination-augmented agents for deep reinforcement learning" from Weber et al. \cite{I2A} combines the advantages of model-free and model-based reinforcement learning to get an agent which is robust against model imperfections, but is able to use the advantages of model-based agents.
The imagination-augmented agent learns therefore to combine information from a model-free and a model-based imagination-augmented path.\\
 
Figure \ref{fig:i2a_architecture} shows the network architecture of the imagination-augmented agent, which will be explained in the following.

\begin{figure}[H] 
  \centering 
   
  \includegraphics[width=\columnwidth]{./Images/i2a_architecture.pdf} 
  \caption{Network architecture for deep reinforcement learning which combines model free and model based reinforcement learning. a) imagination core (IC) predicts the next time step conditioned on an action sampled from the rollout policy $\hat{\pi}$. b) the imagination core imagines trajectories of features f encoded by the rollout encoder. c) the full i2a} 
  \label{fig:i2a_architecture} 
\end{figure}

 
\subsection{Imagination core}


The \textbf{imagination core} (Figure \ref{fig:i2a_architecture} a) imagines trajectories of features $\hat{f}_{t+1} = (\hat{o}_{t+1}, \hat{r}_{t+1})$, containing the next observation $\hat{o}_{t+1}$ and the next reward $\hat{r}_{t+1}$, given the observation $o_t$ or $\hat{o}_{t}$, where $o_t$ refers to the input i2a gets and $\hat{o}_{t}$ refers to an internal state of i2a, which is an output of a previous rollout with the imagination core.
To do so the imagination core uses a rollout policy, which predicts an action given the current state, and an environment model (see chapter \ref{sec:env_model}), which predicts the next observation and the next reward.\\


The \textbf{rollout policy $\hat{\pi}$} (Figure \ref{fig:i2a_architecture} a) is a model-free reinforcement learning agent which should, given the same observation, predict the same action as the i2a policy $\pi$ (Figure \ref{fig:i2a_architecture} c). To ensure this, the rollout policy needs to be similar to the i2a policy $\pi$, which can be ensured by minimizing the distillation loss, the cross entropy between the rollout policy $\hat{\pi}$ and the i2a policy $\pi$:
 
\begin{equation} 
    l_{dist}(\pi, \hat{\pi})(o_t) = \lambda_{dist} \sum_a \pi(a | o_t) log \hat{\pi}(a|o_t) 
\end{equation} 

with scaling parameter $\lambda_{dist}$.\\


\subsection{Environment Model}
\label{sec:env_model}

The environment model (Figure \ref{fig:i2a_architecture} a) predicts (imagines) the next observation $\hat{o}_{t+1}$ and next reward $\hat{r}_{t+1}$, given observation $o_t$ or $\hat{o}_t$ and action $\hat{a}_t$.
The environment model could be any kind of model that fulfills this condition, but in most cases no model of the environment is given.
To solve this problem the environment model is also a trainable neural network.
A trained environment model can not be assumed to be perfect, it might sometimes make wrong predictions, but this does not pose a problem, as the imagination augmented agent is robust to imperfect environment models.

   
\begin{figure}[H] 
  \centering    
  \includegraphics[width=300px]{./Images/i2a_env.pdf}
  \caption{Environment model. The input action is broadcasted and concatenated to the observation. A convolutional neural network then processes the input and predicts the next observation and the expected reward.} 
  \label{fig:environment_model_architecture} 
\end{figure} 

Figure \ref{fig:environment_model_architecture} shows the environment model architecture.
It gets as input the last observation $o_t$ as rgb image and an action $a$, which was selected from the rollout policy $\hat{\pi}$. The action is converted into a one-hot vector and tiled to the size of the observation.
The tiled action is then concatenated with the input observations $o_t$ and named stacked context.
The number of channels of the stacked context is now equal to three rgb channels plus the number of possible actions.
Given the stacked context as input a convolutional neural network with two heads predicts the next observation $o_{t+1}$ and the expected reward $r_{t+1}$.\\

The neural network is trained with transitions of the form $(o_t, a_t) \rightarrow (o_{t+1}, r_{t+1})$ generated from a pretrained model-free advantage-actor-critic policy. This is necessary because a random agent is not able to generate a representative set of states, as it sees few rewards in some of the domains.\\

 
The observation head of the environment model is trained by maximizing the log-likelihood of the probability $p(o_t | a_{t-1}, o_{t-1})$, which is a bernoulli distribution given by

\begin{equation} 
   p(o_t | a_{t-1}, o_{t-1}) = \prod_{n \in I} p_n^{o_{t,n}} (1-p_n)^{1-o_{t,n}} 
\end{equation}

where $p_n$ is the prediction of a single pixel $n\in I$ and $I$ is the set of all pixels in the image corresponding to the true observation $o_t$ at timestep $t$.
%A bernoulli distribution works well because, Basically we pretend that x and \hat x are both a collection of independent bernoulli probabilities (even if they are really just number in between 0 and 1).   
Maximizing the log-likelihood of the probability $p(o_t | a_{t-1}, o_{t-1})$ is the same as the binary cross entropy loss between the true image $o_t$ and the predicted image $p$:


\begin{equation} 
  \mathnormal{ 
  L_{env}(p, o) = \frac{1}{N} \sum_{n \in I} o_{t,n} \log p_n + (1-o_{t,n}) \log(1- p_n) 
  %env_{loss} = Binary Cross Entropy(predicted\_image, true\_images) 
  } 
\end{equation}

To train the predicted reward a L2 loss is used.
The environment model can either pretrained before embedding it within the i2a architecture, or jointly trained with the agent. For our experience we used a pretrained environment model.\\

%TODO the env model can be fine tuned with the i2a
 
\subsection{Rollout Encoder}
 

The rollout encoder processes the imagined rollout as a whole and learns to interpret it, by using any useful information and ignoring information when necessary, see Figure \ref{fig:i2a_architecture}b.
To do so each rollout encoder predicts a imagined trajectory $T = (\hat{f}_{t+1}, ... \hat{f}_{t+n})$, which is a sequence of features, by performing $n$ rollouts, given the input observation $o_t$ and a start action $a_t$.
The encoder consists of a convolutional neural network (CNN) followed by an long-short-term-memory (LSTM) network which processes the information given by the imagined trajectory $T$. By using an LSTM network the encoder is able to learn long-term dependencies in the rollouts.\\

   
 
\subsection{I2A Architecture}


As described above the I2A architecture combines model-free and model-based reinforcement learning (figure \ref{fig:i2a_architecture}c).
The model-based path performs for each action $a$ the agent can take an imagination rollout with the rollout encoder.
The aggregator then concatenates all rollouts.
The model-free path is just a neural network with CNN layers followed by fully connected (FC) layers.
If only the model-free path would be used the architecture would be equal to a model-free reinforcement learning architecture.\\ 

The outputs of the model-free path and the model-based path are then simply concatenated and passed to an output policy network which consists of one FC layer and two heads, the policy head $\pi$ and the value head $V$.
As I2A is just an architecture design it can be trained with advantage-actor-critic as described in section \ref{sec:a2c}.\\

 
\subsection{Model-free baseline}

For the model-free baseline an a2c architecture is used consisting of two convolutional layers followed by a FC layer and two FC output heads, one for the policy and one for the value function.\\

\subsection{Copy Model}

To verify that the improvement of the i2a agent does not rely on the higher model complexity, a copy model is used for comparison. The copy model has the same network architecture as the i2a model but instead of using an environment model a 'copy' model is used which simply returns the input observation. 